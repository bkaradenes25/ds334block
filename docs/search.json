[
  {
    "objectID": "posts/UFOs/UFOs.html",
    "href": "posts/UFOs/UFOs.html",
    "title": "UFOs",
    "section": "",
    "text": "Introduction\nFor my second blog post I am working with a UFO data set from the tidy tuesday library, originally taken from the National UFO Reporting Center. The data includes date, time, location, duration, time of day, and craft shape. I copy pasted US census data into an excel sheet to have state population data. I am most interested in the state, year and time of day variables. My question of interest is how UFO sightings in the USA have trended over time and what time of day and states are most popular. There are 96,429 observations in the data set. SOURCE: https://github.com/rfordatascience/tidytuesday/tree/master/data/2023/2023-06-20\n\n# Read in data from tidy tuesday library\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntuesdata &lt;- tidytuesdayR::tt_load('2023-06-20')\n\n--- Compiling #TidyTuesday Information for 2023-06-20 ----\n--- There are 3 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `ufo_sightings.csv`\n    Downloading file 2 of 3: `places.csv`\n    Downloading file 3 of 3: `day_parts_map.csv`\n\n\n--- Download complete ---\n\ntuesdata &lt;- tidytuesdayR::tt_load(2023, week = 25)\n\n--- Compiling #TidyTuesday Information for 2023-06-20 ----\n--- There are 3 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `ufo_sightings.csv`\n    Downloading file 2 of 3: `places.csv`\n    Downloading file 3 of 3: `day_parts_map.csv`\n\n\n--- Download complete ---\n\nufo_sightings &lt;- tuesdata$`ufo_sightings`\nplaces &lt;- tuesdata$`places`\nday_parts_map &lt;- tuesdata$`day_parts_map`\n\n\n\nPlot 1: Sightings Per State\n\nlibrary(here)\n\nhere() starts at C:/Users/bkara/OneDrive - St. Lawrence University/ds334block\n\nstate_pop &lt;- read.csv(here(\"data/state_pop.csv\"))\n\n\nstate_pop &lt;- state_pop %&gt;%\n  rename(state = \"State\")\n\n\nufo_shape&lt;- ufo_sightings %&gt;%\n  filter(!is.na(shape)) %&gt;%\n  filter(shape != \"unknown\") %&gt;%\n  filter(country_code == \"US\") %&gt;%\n  group_by(state) %&gt;%\n  summarise(nstate = n())\n\n\nufo_shape &lt;- ufo_shape[-11,]\n\n\njoined_state &lt;- left_join(x = state_pop, y = ufo_shape, by = \"state\")\n\n\njoined_state &lt;- joined_state %&gt;%\n  mutate(Population = as.numeric(gsub(\",\", \"\", Population))) %&gt;%\n  mutate(pop_per_hundthou = Population/100000) %&gt;%\n  mutate(per_capita = nstate/pop_per_hundthou)\n\n\nlibrary(usmap)\nplot_usmap(data = joined_state, values = \"per_capita\", color = \"red\") + scale_fill_continuous(name = \"Number of UFO Sightings \\n per 100,000 people\", label = scales::comma, low = \"white\", high = \"blue\") +\ntheme(legend.position = \"right\") +\nlabs(title = \"UFO Sightings in US States per 100,000 people\")+\ntheme(plot.title = element_text(hjust = .65))\n\n\n\n\n\n\nSummary\nThis map is displaying the number of UFO sightings in each US state, adjusted for differing population sizes. Based off of this visualization it seems that no particular region has the most UFO sightings with states fairly evenly spread out from coast to coast. New York has significantly more sightings than any other state per 100,000 people, while Virginia has the second most and significantly higher than the next state. States in white, like Nevada have seen minimal sightings per 100,000 people.\n\n\nPlot 2: Sightings Over Time\n\nufo_sightings$year &lt;- substr(ufo_sightings$reported_date_time, 1, 4)\n\n\nyear_ufo &lt;- ufo_sightings %&gt;%\n  filter(country_code == \"US\") %&gt;%\n  group_by(year) %&gt;%\n  summarise(nyear = n()) \n\n\nggplot(data = year_ufo, aes(x = year, y = nyear)) +\ngeom_point(color = \"red\") +\ntheme_minimal() + \nlabs(title = \"Number of UFO sightings over time in USA\", y = \"Number of Sightings\") +\nscale_x_discrete(breaks = seq(1920, 2030, by = 10)) +\ntheme(plot.title = element_text(hjust = .25))\n\n\n\n\n\n\nSummary\nThe scatterplot above displays the number of UFO sightings in the USA over the past 100 years. The amount of sightings stayed fairly constant with a minimal amount of sightings until the mid-1990s when there began a steady increase in sightings until the mid 2010s. The amount of sightings peaked in 2014, with the number of sightings undergoing a sudden drop after. The amount of sightings has been decreasing from the peak in 2014 to 2023.\n\n\nPlot 3: Sightings in Time of Day\n\ntime &lt;- ufo_sightings %&gt;%\n  filter(!is.na(day_part)) %&gt;%\n  filter(country_code == \"US\") %&gt;%\n  group_by(day_part) %&gt;%\n  summarise(nday_part = n()) %&gt;%\n  mutate(day_part = fct_reorder(day_part, nday_part))\n\n\nggplot(data = time, aes(x = day_part, y = nday_part)) +\ngeom_segment(aes(xend = day_part, yend = 0)) +\ngeom_point(size = 4, color = \"red\") +\ncoord_flip() +\ntheme_minimal() +\nlabs(title = \"Number of UFO Sightings in USA per Time of Day\",y = \"Number of Sightings\", x = \"Time of Day\")\n\n\n\n\n\n\nSummary\nThe graph above displays the number of UFO sightings in the United States for each time of day. UFO sightings at night have significantly more sightings than any other time of day, while times around dawn have the least amount of sightings. For reference, astronomical is the earliest and civil is the latest times for dawn and the opposite for dusk. It seems when the sky is darker there are more sightings with night, afternoon, and astronomical dusk having the most sightings.\n\n\nConclusion\nGiven the nature of the data I chose, it’s difficult to tell whether each observation was actually thought to be seen by someone or if they were just completely making it up. To address this I would have looked more carefully at the summary variables for each one, but there are so many observations that would be far too time consuming. If I had more time I would have looked into why there was such a large spike in the mid-1990s and why New York has so many more sightings per capita than any other state. I think it would be interesting to look into which UFO movies are based in each state or how many airports/military bases each state has that might make people think they are seeing UFOs in the sky.\n\n\nClass Ideas\nFor the first graph I chose a map because the data was geographical and since there are 50 states + DC it seemed the best way to eliminate clutter. You could also see the fairly even spread of sightings per capita from coast to coast that you wouldn’t be able to tell if I had used a graph. I chose a scatterplot in the second plot because it is the most efficient way to represent 2 quantitative variables. You could easily see the trend and the sharp increase in sightings from the mid-1990s and into the 2000s. For the third graph I chose a lollipop plot because I was representing counts in the number of sightings and although lollipop plots aren’t always the best way to represent data it is easy to compare the different times of day in this case."
  },
  {
    "objectID": "posts/College_Sports_Budgets/Sports-Budgets.html",
    "href": "posts/College_Sports_Budgets/Sports-Budgets.html",
    "title": "College_Sports_Budgets",
    "section": "",
    "text": "Introduction\nFor my third blog post, I am working with a collegiate sports data set that contains 132,327 observations of 28 variables. I am most interested in the expenditure, revenue, and sports variables. The data is from 2015 to 2019. I got the data from the tidy tuesday GitHub and it is originally from the US Department of Education’s Equity in Athletics Data Analysis website. My questions of interest is which sports are the most expensive, which are played the most, and what is the general national trend of expenditure and revenue in college sports.\nSource: https://ope.ed.gov/athletics/#/datafile/list\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-03-29')\n\n--- Compiling #TidyTuesday Information for 2022-03-29 ----\n\n\n--- There is 1 file available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 1: `sports.csv`\n\n\n--- Download complete ---\n\ntuesdata &lt;- tidytuesdayR::tt_load(2022, week = 13)\n\n--- Compiling #TidyTuesday Information for 2022-03-29 ----\n\n\n--- There is 1 file available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 1: `sports.csv`\n\n\n--- Download complete ---\n\nsports &lt;- tuesdata$sports\n\n\n\nVisualization 1: Expenditure vs Revenue\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nsports_noNA &lt;- sports %&gt;%\n  filter(!is.na(total_exp_menwomen)) %&gt;%\n  filter(!is.na(total_rev_menwomen))\n\n\nggplot(data = sports_noNA, aes(x = total_exp_menwomen, y = total_rev_menwomen)) +\n  geom_hex() +\n  scale_fill_viridis_c() +\n  theme_minimal() +\n  labs(x = \"Total Expenditure\", y = \"Total Revenue\", title = \"Expenditure vs. Revenue for all Collegiate Sports in the USA\")\n\n\n\n\n\n\nSummary\nThe plot above compares the total expenditure and total revenue for men and woman’s college sports teams in the USA. A large proportion of the schools fall close to zero in both categories (spending little and making little money from the school’s sports). However, there is a general positive trend as total expenditure increases, total revenue also increases. As expenditure gets higher the relationship gets weaker.\n\n\nVisualization 2: Most Popular College Sports\n\npop_sports &lt;- sports %&gt;%\n  group_by(sports) %&gt;%\n  summarise(nsports = n()) %&gt;%\n  mutate(sports = fct_reorder(sports, nsports)) %&gt;%\n  filter(nsports &gt; 5000)\n\n\nggplot(data = pop_sports, aes(x = sports, y = nsports)) +\ngeom_segment(aes(x = sports, xend = sports, y = 0, yend = nsports)) +\ngeom_point(size = 3) +\ntheme_minimal() +\ncoord_flip() + \nlabs(y = \"Number of Colleges with Sport\", x = \"Sports\", title = \"Sports Played at 5000 or more Colleges\") +\ntheme(plot.title = element_text(hjust = 0.4))\n\n\n\n\n\n\nSummary\nThe plot above displays the 8 most popular sports played at American colleges from 2015 to 2019, all of which are played at 5000 or more colleges. The top 3 sports (Basketball, Volleyball, Soccer) are all co-ed with baseball, softball, and football being the only sports without both mens and womans teams. Basketball nearly doubles the number of football teams, while soccer, baseball, and soccer are played at close to the same number of colleges.\n\n\nVisualization 3: Sports with Most Total Expendenture\n\ncollege_sports &lt;- sports %&gt;%\n  group_by(sports) %&gt;%\n  summarise(total_exp_menwomen = sum(total_exp_menwomen, na.rm = TRUE)) %&gt;%\n  filter(total_exp_menwomen &gt; 2000000000) %&gt;%\n  mutate(sports = fct_reorder(sports, total_exp_menwomen))\n\n\nggplot(data = college_sports, aes(x = sports, y = total_exp_menwomen)) +\ngeom_segment(aes(x = sports, xend = sports, y = 0, yend = total_exp_menwomen)) +\ngeom_point(size = 3) +\ntheme_minimal() +\ncoord_flip() + \nlabs(y = \"Total Expenditures ($)\", x = \"Sports\", title = \"College Sports with Highest Total Expenditures\") +\ntheme(plot.title = element_text(hjust = 0.4))\n\n\n\n\n\n\nSummary\nThe plot above displays the top 8 college sports with highest total expenditures. Interestingly, although football was the eighth most played sport, it has the highest expenditure by a significant margin. Only basketball is close and it has almost double the teams, as we saw in the previous plot. The other 6 sports are fairly close together in expenditures and of the 8 most popular sports, 7 are in the top 8 in expenditures with Track replacing Golf.\n\n\nConclusion\nOne of the flaws in the way I approached the data was that I didn’t separate into men’s and woman’s sports because they were combined into one column if they both played the same sport. It was difficult to separate them because the data didn’t have a gender variable and placed them under the same column. If I had more time/additional data I would look into the differential in expenditure between men’s and woman’s sports at these colleges. I would also like to look at St. Lawrence’s data and how it compares on a national scale. Also, I think comparing within NCAA divisions would be interesting as well.\n\n\nClass Ideas\nFor the first plot, that was an example of working with large data. Since there were so many observations I used geom_hex and a continuous scale to map the number of points to a count fill aesthetic. This made it easier to view and interpret the data, instead of looking at a bunch of points piled together with the same color. For the last two plots I chose line plots. Given the nature of the data of using 8 different levels to the sports variables and having a sum of the quantitative variables, it seemed the best way to compare the two. Since they were the same type of graphic it makes them easier to compare the two and make a possible connection between number of teams and expenditures."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog was created in the Spring of 2024, during my second semester of junior year at St. Lawrence University. I created it for my Data Visualization class to show graphics of various data sets."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visualizing Data",
    "section": "",
    "text": "College_Sports_Budgets\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2024\n\n\nBrendan Karadenes\n\n\n\n\n\n\n  \n\n\n\n\nUFOs\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nBrendan Karadenes\n\n\n\n\n\n\n  \n\n\n\n\nTornados\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nBrendan Karadenes\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Tornados/Tornado.html",
    "href": "posts/Tornados/Tornado.html",
    "title": "Tornados",
    "section": "",
    "text": "Introduction\nFor this blog post I am working with a data set from NOAA National Weather Service Storm Prediction Center Severe Weather Maps, Graphics, and Data Page, taken from the tidy tuesday github. The data includes an id variable for each tornado, dates, times, fatalities, area, and state information. I am most interested in the year, property damage, length, width, and state variables. There are 68693 observations in the data set. My question of interest is to find out how tornadoes have affected the USA from the period 1950-2021 and which states were hit the hardest.\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nhere() starts at C:/Users/bkara/OneDrive - St. Lawrence University/ds334block\n\ntornados &lt;- read.csv(here(\"data/tornados.csv\"))\n\n\n\nPlot 1: Area Covered Through the Years\n\n# create area variable\ndamage_tornados &lt;-\n  tornados %&gt;%\n  mutate(length_yds = len*1760) %&gt;%\n  mutate(area = len*wid) %&gt;%\n  filter(area &gt; 0) %&gt;%\n  group_by(yr) %&gt;%\n  summarise(area = sum(area))\n# plot year vs area graph\nggplot(data = damage_tornados, mapping = aes(x = yr, y = area)) + geom_point(color = \"red\") + geom_smooth() + theme_minimal() + labs(title = \"Area Covered by Tornadoes in the USA from 1950-2021\", x = \"Year\", y = \"Area (sq. miles)\") + theme(plot.title = element_text(hjust = .4))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nSummary\nIn the scatterplot above, I plotted year versus area to see if total tornado area has changed during the time period from 1950-2021. According to the graph, it seems area remained roughly the same from 1950 to the mid 1990s, where area per year began a slight rise until 2021. The year 2011 seems to be an outlier, with a total tornado area of over 2,000,000 square miles above the next closest year.\n\n\nPlot 2: Tornado Property Damage from 1950-2021\n\n# get property damage per year\nproperty_tornados &lt;-\n  tornados %&gt;%\n  filter(!is.na(loss)) %&gt;%\n  group_by(yr) %&gt;%\n  summarise(tot_damage = sum(loss)) %&gt;%\n  mutate(tot_damage = tot_damage/1000000)\n# graph year vs loss\nggplot(data = property_tornados, mapping = aes(x = yr, y = tot_damage)) + geom_point(color = \"red\") + geom_smooth(color = \"green\") +   theme_minimal() + labs(title = \"Property Damage by Tornadoes in the USA from 1950-2021\", x = \"Year\", y = \"Damage (Millions of Dollars)\") + theme(plot.title = element_text(hjust = .4))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nSummary\nThe scatterplot above displays the relationship between year and property damage in millions of dollars from tornadoes. There seems to be an increase in property damage during the mid 1960s for about 20 years until the curve flattens out again until 2021. Again, perhaps due to the large area covered by tornadoes in 2011, the year seems to be an outlier in this relationship as well, with more than twice the property damage than the next closest year.\n\n\nPlot 3: Map of Property Damage By State\n\ntornado_mapData &lt;-\ntornados %&gt;%\n  group_by(st) %&gt;%\n  filter(!is.na(loss)) %&gt;%\n  summarise(damage_bystate = sum(loss)) %&gt;%\n  mutate(state = st)\n\n\n# https://cran.r-project.org/web/packages/usmap/index.html\nlibrary(usmap)\nplot_usmap(data = tornado_mapData, values = \"damage_bystate\", color = \"red\") + scale_fill_continuous(name = \"Property Damage ($)\", label = scales::comma, low = \"white\", high = \"green\") + theme(legend.position = \"right\") + labs(title = \"Tornado Property Damage in US States\") + theme(plot.title = element_text(hjust = .65))\n\n\n\n\nThe map above displays property damage by US state. During the period from 1950-2021, Texas, Alabama, and Oklaholma sustained the most property damage in dollars from tornadoes. Texas by far sustained the most significant damage with over $10 billion in losses over the 71 year period. Most of the states on the southeastern coast sustained significant damage, while west coast states suffered the least.\n\n\nConclusion\nOne of the flaws would be I never looked into the trends in the number of tornadoes over time, which could be part of the reason why there is a larger area covered by tornadoes in recent years or be a reason for an increase in property damage. I am also unsure about the accuracy in the data collection 70 years ago compared to today, there could be inaccurate data points that affect the way I analyzed the data. If I had more data/time I would look into how the tornadoes affected the economies of the states that got hit the hardest. I would also see how economics were affected in 2011, which was the year with by far the most property damage.\n\n\nClass Ideas\nThe first two visualizations I chose to represent the data were scatterplots because you’re able to see each point data point. This allows you to see the exact trend of the data and not just general summary statistics. They are also simple scatterplots that show the data and their trends. I didn’t want to include any extra aestetic parts that might obsure the trends and make it difficult to read. I think they do a good job of maximizing the data-to-ink ratio, while still adding appropriate labels. For the map graph, I used a continuous scale to represent the property damage becuase the amount of money goes from low to high. I also thought the green color complimented the red well, making it easier to see. I chose a map becuase I thought it would be a good way to show what regions of the country were affected the most to see what areas suffered the most property damage. I also think a map would be more memorable for the viewer than another plot."
  }
]